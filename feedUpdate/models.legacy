# Legacy code
# (no longer works as AJAX is used at ранобэ.рф)
def import_ranoberf(self):
    resp = requests.get(feedUpdate.books[book]['href'])  # 0.4 seconds
    strainer = SoupStrainer('div', attrs={'class': 'col-md-12'});
    soup = BeautifulSoup(resp.text, "html.parser")  # ~0.4 Sculptor / ~0.7 System seconds

    print(str(resp), str(soup), requests.get(feedUpdate.books[book]['href']))

    chapter_names = []
    chapter_datetimes = []
    chapter_links = []

    for entry in soup.find_all('a'):
        if str(entry).find('strong') != -1:
            chapter_names.append(entry.text)

    for entry in soup.find_all("time"):
        chapter_datetimes.append(datetime.strptime(entry.get('datetime')[:-6], "%Y-%m-%dT%H:%M:%S"))

    for entry in soup.find_all('a'):
        entry = entry.get('href')
        if type(entry) == str:
            if entry.find(feedUpdate.books[book]['href']) != -1:  # checking if link leads to the same website
                chapter_links.append(entry)
    # chapter_links.pop(0)  # it is the button in the begging "Start reading"
    chapter_links = list(OrderedDict((x, True) for x in chapter_links).keys())  # allow unique links only
    if len(chapter_links) == len(chapter_names) and len(chapter_names) == len(chapter_datetimes):
        for i in range(0, len(chapter_links)):
            result.append(feedUpdate(
                name=str(chapter_names[i]),
                href=str(chapter_links[i]),
                datetime=str(chapter_datetimes[i]),
                title=book))

    else:
        print("Number of links (%(links)s) do not match titles (%(names)s) and datetimes (%(datetimes)s)"
              % {'links': len(chapter_links), 'names': len(chapter_names), 'datetimes': len(chapter_datetimes)})

        def import_ReflectiveDesire_01(self):
            feedName = "Reflective"
            result = []

            if feedUpdate.feeds[feedName]['href'].find('reflectivedesire.com/rss/') != -1:
                feed = feedparser.parse(feedUpdate.feeds[feedName]['href'])
                for item in feed["items"]:
                    result.append(feedUpdate(
                        name=item["title_detail"]["value"],
                        href=item["links"][0]["href"],
                        datetime=datetime.strptime(item["published"], '%a, %d %b %Y %H:%M:%S -0700'),
                        title=feedName))

def import_ReflectiveDesire_02(self):
    feedName = "Reflective"
    result = []

    # RSS reflectivedesire.com import manual workaround
    if feedUpdate.feeds[feedName]['href'].find('reflectivedesire.com/rss/') != -1:
        try:
            resp = requests.get(feedUpdate.feeds[feedName]['href'])
            soup = BeautifulSoup(resp.text, "html.parser")

            log = feedUpdate(
                name="SUCCESS",
                href="#",
                datetime=str(datetime.now().strftime("%Y-%m-%d %H:%M:%S")),
                title=feedName)
            log.save()
            # result.append(log)
            # print(log)

            for each in soup.find_all("item"):
                result.append(feedUpdate(
                    name=each.find("title").string,
                    href=each.find("guid").string,
                    datetime=datetime.strptime(each.find("pubdate").string, '%a, %d %b %Y %H:%M:%S -0700'),
                    title=feedName))
        except requests.exceptions.ConnectionError:
            print("feedUpdate/feeds/Reflective: Connection refused")

def import_youtube_alt(self):
    # YouTube import ALTERNATIVE (https://www.youtube.com/feeds/videos.xml?channel_id=)
    result = []
    feedName = ""

    if feedUpdate.feeds[feedName]['href'].find(
            'https://www.youtube.com/feeds/videos.xml?channel_id=') != -1:
        feed = feedparser.parse(feedUpdate.feeds[feedName]['href'])

        for item in feed["items"]:
            result.append(feedUpdate(
                name=item["title"],
                href=item["link"],
                datetime=datetime.strptime(item["published"], '%Y-%m-%dT%H:%M:%S+00:00'),
                title=feedName))

def import_youtube_alt_x2(self):
    # custom RSS YouTube import (link to feed has to be converted manually)
    elif href.find('https://www.youtube.com/channel/') != -1:
        feed = feedparser.parse("https://www.youtube.com/feeds/videos.xml?channel_id="+href[32:-7])

        for item in feed["items"]:
            result.append(feedUpdate(
                name=item["title"],
                href=item["link"],
                datetime=datetime.strptime(item["published"], '%Y-%m-%dT%H:%M:%S+00:00')+timedelta(hours=3),
                title=feedName))

def import_novelupdates(self):
    # custom novelupdates.com import

    elif href.find('https://www.novelupdates.com/series/') != -1:
    result = []
    result_name = []
    result_href = []
    result_datetime = []

    resp = requests.get(href)  # 0.4 seconds
    strainer = SoupStrainer('table', attrs={'id': 'myTable'});
    soup = BeautifulSoup(resp.text, "lxml", parse_only=strainer)  # ~0.4 Sculptor / ~0.7 System seconds

    for entry in soup.find_all(attrs={"class": "chp-release"}):
        result_name.append("Chapter "+entry['title'][1:])
        result_href.append("http:"+entry['href'])

    for entry in soup.find_all(attrs={"style": "padding-left:5px;"}):
        if entry.text != "Date":
            result_datetime_time=timedelta(
                hours=datetime.now().hour,
                minutes=datetime.now().minute,
                seconds=datetime.now().second)
            #if datetime.now().hour <= 12:
            #    result_datetime_time = result_datetime_time+timedelta(days=1)
            # +timedelta(hours=3)

            result_datetime_time = datetime.strptime(entry.text, "%m/%d/%y")+result_datetime_time
            result_datetime_time = result_datetime_time + timedelta(hours=24)
            # result_datetime_time = datetime.now()
            result_datetime_time.astimezone(timezone('Europe/Kiev'))
            result_datetime.append(result_datetime_time)
            result_datetime = feedUpdate.datetimeModifier(result_datetime)

    if len(result_name) == len(result_href) and len(result_href) == len(result_datetime):
        for num in range(0, len(result_name)):
            result.append(feedUpdate(
                name=result_name[num],
                href=result_href[num],
                datetime=result_datetime[num],
                title=feedName))
